{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "This final project can be collaborative. The maximum members of a group is 3. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints). You can freely determine every component of your workflow, including but not limited to:\n",
    "-  **Preprocessing the input text**: You may decide how to clean or transform the text. For example, removing emojis or URLs, lowercasing, removing stopwords, applying stemming or lemmatization, correcting spelling, or performing tokenization and sentence segmentation.\n",
    "-  **Feature extraction and encoding**: You can choose any method to convert text into numerical representations, such as TF-IDF, Bag-of-Words, N-grams, Word2Vec, GloVe, FastText, contextual embeddings (e.g., BERT, RoBERTa, or other transformer-based models), Part-of-Speech (POS) tagging, dependency-based features, sentiment or emotion features, readability metrics, or even embeddings or features generated by large language models (LLMs).\n",
    "-  **Data augmentation and enrichment**: You may expand or balance your dataset by incorporating other related corpora or using techniques like synonym replacement, random deletion/insertion, or LLM-assisted augmentation (e.g., generating paraphrased or synthetic examples to improve model robustness).\n",
    "-  **Model selection**: You are free to experiment with different models — from traditional machine learning algorithms (e.g., Logistic Regression, SVM, Random Forest, XGBoost) to deep learning architectures (e.g., CNNs, RNNs, Transformers), or even hybrid/ensemble approaches that combine multiple models or leverage LLM-generated predictions or reasoning.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values. You may explore both traditional and AI-assisted techniques. Data augmentation is optional.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision (P), Recall (R) and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. Here is an example illustrating how the experimental results table should be presented.\n",
    "\n",
    "| Feature + Model | Sexist (P) | Sexist (R) | Sexist (F1) | Non-Sexist (P) | Non-Sexist (R) | Non-Sexist (F1) | Weighted (P) | Weighted (R) | Weighted (F1) |\n",
    "|-----------------|:----------:|:----------:|:------------:|:---------------:|:---------------:|:----------------:|:-------------:|:--------------:|:---------------:|\n",
    "| TF-IDF + Logistic Regression | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
    "\n",
    "- **Format of the report**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary for each sections: \n",
    "    - Data Preprocessing\n",
    "    - Feature Engineering\n",
    "    - Model Selection and Architecture\n",
    "    - Training and Validation\n",
    "    - Evaluation and Results\n",
    "    - Use of Generative AI (if you use)\n",
    "\n",
    "## Rules \n",
    "Violations will result in 0 points in the grade: \n",
    "-   `Rule 1 - No test set leakage`: You must not use any instance from the test set during training, feature engineering, or model selection.\n",
    "-   `Rule 2 - Responsible AI use`: You may use generative AI, but you must clearly document how it was used. If you have used genAI, include a section titled “Use of Generative AI” describing:\n",
    "    -   What parts of the project you used AI for\n",
    "    -   What was implemented manually vs. with AI assistance\n",
    "\n",
    "## Grading\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance reaches **0.82** or above (weighted F1-score) and follows all the requirements and rules, you will also get full points (10.0 points). \n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report including: \n",
    "- code and experimental results with details explained\n",
    "- combined results table, report and best performance\n",
    "- a summary at the end of the report (please follow the format above)\n",
    "\n",
    "Missing any part of the above requirements will result in point deductions.\n",
    "\n",
    "The due date is **Dec 11, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156cb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3da4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4feddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8675e93",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "\n",
    "(A table detailed model performance on the test set with at least 6 rows. Report the best performance.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f84b04",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "\n",
    "### 2. Feature Engineering\n",
    " \n",
    "\n",
    "### 3. Model Selection and Architecture\n",
    "\n",
    "\n",
    "### 4. Training and Validation\n",
    "\n",
    "\n",
    "### 5. Evaluation and Results\n",
    "\n",
    "\n",
    "### 6. Use of Generative AI (if you use)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242656d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5bfbd2c",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0491a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "# Load dataset\n",
    "df = pd.read_csv('edos_labelled_data.csv')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def advanced_clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\[user\\]|\\[url\\]\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    \n",
    "    # Keep some punctuation that might be meaningful for sexism detection\n",
    "    text = re.sub(r\"[^a-z\\s!?.]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # Optional: lemmatization (can help or hurt, test it)\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "df[\"clean_text\"] = df[\"text\"].apply(advanced_clean_text)\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "test_df = df[df[\"split\"] == \"test\"]\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c53372",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c22918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word features: 16211\n",
      "Char features: 15000\n",
      "Combined features: 31211\n"
     ]
    }
   ],
   "source": [
    "# Going to use TF-IDF vectorization for text features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1,3),\n",
    "    max_features=20000,\n",
    "    min_df=2,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "# Method 2: Character-level TF-IDF (CRUCIAL - this is your second method!)\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(3,5),\n",
    "    max_features=15000,\n",
    "    min_df=3,\n",
    "    max_df=0.90,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit both\n",
    "X_train_word = word_vectorizer.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_word = word_vectorizer.transform(test_df[\"clean_text\"])\n",
    "\n",
    "X_train_char = char_vectorizer.fit_transform(train_df[\"clean_text\"])\n",
    "X_test_char = char_vectorizer.transform(test_df[\"clean_text\"])\n",
    "\n",
    "# Combine them (this will be your best feature set)\n",
    "X_train_combined = hstack([X_train_word, X_train_char])\n",
    "X_test_combined = hstack([X_test_word, X_test_char])\n",
    "\n",
    "print(f\"Word features: {X_train_word.shape[1]}\")\n",
    "print(f\"Char features: {X_train_char.shape[1]}\")\n",
    "print(f\"Combined features: {X_train_combined.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d74334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original class distribution:\n",
      "label\n",
      "not sexist    2934\n",
      "sexist        1259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced class distribution:\n",
      "label\n",
      "not sexist    2934\n",
      "sexist        2934\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we used AI for this part btw so will have to cite that but what it does is:\n",
    "# for each minority sample, it finds k nearest neighbors and generates synthetic samples\n",
    "# it will interpolate new points between knn and then it operates in the numeric feature space\n",
    "# so it assumes interpolation yields meaningful examples\n",
    "# and all this reduces bias towards majority class and helps improve recall and F1 for minority class\n",
    "# using this helped me get to 0.81 weighted F1 score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_combined, y_train)\n",
    "\n",
    "print(f\"\\nOriginal class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nBalanced class distribution:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ec1bbf-2a08-4d92-a3bd-7c2a6cc2cc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#THIS CODE ONLY NEEDED TO BE RUN ONCE TO FIND THE OPTIMAL PARAMETERS FOR XGBoost IT TOOK AROUND 6 HOURS\\n#Before we turn it in I can run it overnight to make the output look nice\\n\\n\\n#GridSearch\\nfrom sklearn.model_selection import GridSearchCV\\nimport xgboost as xgb\\nfrom sklearn.metrics import f1_score, make_scorer\\n\\n# define the parameters to search\\nparam_grid = {\\n    \\'n_estimators\\': [200, 300, 400],  \\n    \\'max_depth\\': [6, 7, 8],          \\n    \\'learning_rate\\': [0.1, 0.05],     \\n    \\'subsample\\': [0.8, 0.9],          \\n    \\'colsample_bytree\\': [0.8, 1.0]   \\n}\\n\\n#inits XGBoost model\\nxgb_base = xgb.XGBClassifier(\\n    random_state=42,\\n    eval_metric=\\'logloss\\',\\n    use_label_encoder=False\\n)\\n\\n#makes weighted f1 the scoring metric\\nf1_scorer = make_scorer(f1_score, average=\\'weighted\\')\\n\\n# 4. init GridSearchCV\\ngrid_search = GridSearchCV(\\n    estimator=xgb_base,\\n    param_grid=param_grid,\\n    scoring=f1_scorer,\\n    cv=3,\\n    verbose=1,\\n    n_jobs=-1\\n)\\n\\nprint(\"starting gridsearch (this takes a very long time)\")\\n\\ngrid_search.fit(X_train_balanced, y_train_balanced_encoded)\\n\\nprint(f\"Best CV Weighted F1 Score: {grid_search.best_score_:.2f}\")\\nprint(f\"Best Parameters Found: {grid_search.best_params_}\")\\n\\n\\n#get the best estimator\\nbest_xgb = grid_search.best_estimator_\\n\\ny_pred_xgb_best_encoded = best_xgb.predict(X_test_combined)\\ny_pred_xgb_best = le.inverse_transform(y_pred_xgb_best_encoded)\\n\\n# bestXGB f1score\\nfinal_f1 = f1_score(y_test, y_pred_xgb_best, average=\\'weighted\\')\\n\\nprint(classification_report(y_test, y_pred_xgb_best))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#THIS CODE ONLY NEEDED TO BE RUN ONCE TO FIND THE OPTIMAL PARAMETERS FOR XGBoost IT TOOK AROUND 6 HOURS\n",
    "#Before we turn it in I can run it overnight to make the output look nice\n",
    "\n",
    "\n",
    "#GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "# define the parameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 400],  \n",
    "    'max_depth': [6, 7, 8],          \n",
    "    'learning_rate': [0.1, 0.05],     \n",
    "    'subsample': [0.8, 0.9],          \n",
    "    'colsample_bytree': [0.8, 1.0]   \n",
    "}\n",
    "\n",
    "#inits XGBoost model\n",
    "xgb_base = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "#makes weighted f1 the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# 4. init GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"starting gridsearch (this takes a very long time)\")\n",
    "\n",
    "grid_search.fit(X_train_balanced, y_train_balanced_encoded)\n",
    "\n",
    "print(f\"Best CV Weighted F1 Score: {grid_search.best_score_:.2f}\")\n",
    "print(f\"Best Parameters Found: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "#get the best estimator\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "y_pred_xgb_best_encoded = best_xgb.predict(X_test_combined)\n",
    "y_pred_xgb_best = le.inverse_transform(y_pred_xgb_best_encoded)\n",
    "\n",
    "# bestXGB f1score\n",
    "final_f1 = f1_score(y_test, y_pred_xgb_best, average='weighted')\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb_best))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65612a78",
   "metadata": {},
   "source": [
    "Model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model combinations\n",
      "Started on: Word TF-IDF + Logistic Regression\n",
      "Finished with: Word TF-IDF + Logistic Regression\n",
      "Started on: Word TF-IDF + SVM\n",
      "Finished with: Word TF-IDF + SVM\n",
      "Started on: Char TF-IDF + Logistic Regression\n",
      "Finished with: Char TF-IDF + Logistic Regression\n",
      "Started on: Char TF-IDF + SVM\n",
      "Finished with: Char TF-IDF + SVM\n",
      "Started on: Combined (Word+Char) + Logistic Regression\n",
      "Finished with: Combined (Word+Char) + Logistic Regression\n",
      "Started on: Combined (Word+Char) + SVM\n",
      "Finished with: Combined (Word+Char) + SVM\n",
      "Started on: Combined + SMOTE + Logistic Regression\n",
      "Finished with: Combined + SMOTE + Logistic Regression\n",
      "Started on: Combined + SMOTE + SVM\n",
      "Finished with: Combined + SMOTE + SVM\n",
      "Started on: Word TF-IDF + XGBoost\n",
      "Finished wish: Word TF-IDF + XGBoost\n",
      "Started on: Char TF-IDF + XGBoost\n",
      "Finished wish: Char TF-IDF + XGBoost\n",
      "Started on: Combined (Word+Char) + XGBoost\n",
      "Finished wish: Combined (Word+Char) + XGBoost\n",
      "Started on: Combined + SMOTE + XGBoost\n",
      "Finished wish: Combined + SMOTE + XGBoost\n",
      "Finished with all combos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature + Model</th>\n",
       "      <th>Sexist (P)</th>\n",
       "      <th>Sexist (R)</th>\n",
       "      <th>Sexist (F1)</th>\n",
       "      <th>Not Sexist (P)</th>\n",
       "      <th>Not Sexist (R)</th>\n",
       "      <th>Not Sexist (F1)</th>\n",
       "      <th>Weighted (P)</th>\n",
       "      <th>Weighted (R)</th>\n",
       "      <th>Weighted (F1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word TF-IDF + Logistic Regression</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Word TF-IDF + SVM</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Char TF-IDF + Logistic Regression</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Char TF-IDF + SVM</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combined (Word+Char) + Logistic Regression</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Combined (Word+Char) + SVM</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Combined + SMOTE + Logistic Regression</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Combined + SMOTE + SVM</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Word TF-IDF + SVM</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Char TF-IDF + SVM</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Combined (Word+Char) + SVM</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Combined + SMOTE + SVM</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Feature + Model Sexist (P) Sexist (R)  \\\n",
       "0            Word TF-IDF + Logistic Regression       0.82       0.24   \n",
       "1                            Word TF-IDF + SVM       0.73       0.48   \n",
       "2            Char TF-IDF + Logistic Regression       0.84       0.31   \n",
       "3                            Char TF-IDF + SVM       0.73       0.45   \n",
       "4   Combined (Word+Char) + Logistic Regression       0.79       0.38   \n",
       "5                   Combined (Word+Char) + SVM       0.74       0.54   \n",
       "6       Combined + SMOTE + Logistic Regression       0.64       0.66   \n",
       "7                       Combined + SMOTE + SVM       0.64       0.61   \n",
       "8                            Word TF-IDF + SVM       0.76       0.47   \n",
       "9                            Char TF-IDF + SVM       0.76       0.48   \n",
       "10                  Combined (Word+Char) + SVM       0.73       0.48   \n",
       "11                      Combined + SMOTE + SVM       0.79       0.52   \n",
       "\n",
       "   Sexist (F1) Not Sexist (P) Not Sexist (R) Not Sexist (F1) Weighted (P)  \\\n",
       "0         0.37           0.77           0.98            0.86         0.79   \n",
       "1         0.58           0.83           0.93            0.88         0.80   \n",
       "2         0.45           0.79           0.98            0.87         0.80   \n",
       "3         0.56           0.82           0.94            0.87         0.79   \n",
       "4         0.52           0.81           0.96            0.88         0.80   \n",
       "5         0.62           0.84           0.93            0.88         0.81   \n",
       "6         0.65           0.87           0.86            0.86         0.81   \n",
       "7         0.62           0.86           0.87            0.86         0.79   \n",
       "8         0.58           0.83           0.94            0.88         0.81   \n",
       "9         0.59           0.83           0.94            0.88         0.81   \n",
       "10        0.58           0.83           0.93            0.88         0.80   \n",
       "11        0.62           0.84           0.95            0.89         0.82   \n",
       "\n",
       "   Weighted (R) Weighted (F1)  \n",
       "0          0.78          0.73  \n",
       "1          0.81          0.80  \n",
       "2          0.79          0.76  \n",
       "3          0.80          0.79  \n",
       "4          0.80          0.78  \n",
       "5          0.82          0.81  \n",
       "6          0.80          0.80  \n",
       "7          0.80          0.80  \n",
       "8          0.81          0.80  \n",
       "9          0.82          0.80  \n",
       "10         0.81          0.80  \n",
       "11         0.83          0.82  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Combo: Combined + SMOTE + XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.84      0.95      0.89       789\n",
      "      sexist       0.79      0.52      0.62       297\n",
      "\n",
      "    accuracy                           0.83      1086\n",
      "   macro avg       0.81      0.73      0.76      1086\n",
      "weighted avg       0.82      0.83      0.82      1086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(np.unique(y_train)) \n",
    "y_train_balanced_encoded = le.transform(y_train_balanced)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "#Best Tracker\n",
    "best_result = [0.0, \"None\", None, None] \n",
    "\n",
    "# Define feature sets\n",
    "feature_sets = [\n",
    "    (\"Word TF-IDF\", X_train_word, X_test_word, y_train, y_test),\n",
    "    (\"Char TF-IDF\", X_train_char, X_test_char, y_train, y_test),\n",
    "    (\"Combined (Word+Char)\", X_train_combined, X_test_combined, y_train, y_test),\n",
    "    (\"Combined + SMOTE\", X_train_balanced, X_test_combined, y_train_balanced, y_test),\n",
    "]\n",
    "\n",
    "# Define base models\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=5000, C=1.0, random_state=42, solver='saga')), \n",
    "    (\"SVM\", LinearSVC(C=0.5, max_iter=2000, random_state=42)),\n",
    "]\n",
    "\n",
    "print(\"running model combinations\")\n",
    "\n",
    "# LR and SVM Loop\n",
    "for feat_name, X_tr, X_te, y_tr, y_te in feature_sets:\n",
    "    for model_name, model in models:\n",
    "        full_name = f\"{feat_name} + {model_name}\"\n",
    "\n",
    "        print(f\"Started on: {full_name}\")\n",
    "        \n",
    "        y_true_final = y_test # The true labels are always y_test\n",
    "        try:\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_te)\n",
    "            \n",
    "            f1_weighted = f1_score(y_true_final, y_pred, average='weighted')\n",
    "\n",
    "            #Print the current report\n",
    "            report = classification_report(y_true_final, y_pred)\n",
    "            \n",
    "            \n",
    "            #This is for the dataframe\n",
    "            reportDict = classification_report(y_true_final, y_pred, output_dict=True)\n",
    "            results_list.append({\n",
    "                \"Feature + Model\": f\"{feat_name} + {model_name}\",\n",
    "                \"Sexist (P)\": f\"{reportDict['sexist']['precision']:.2f}\",\n",
    "                \"Sexist (R)\": f\"{reportDict['sexist']['recall']:.2f}\",\n",
    "                \"Sexist (F1)\": f\"{reportDict['sexist']['f1-score']:.2f}\",\n",
    "                \"Not Sexist (P)\": f\"{reportDict['not sexist']['precision']:.2f}\",\n",
    "                \"Not Sexist (R)\": f\"{reportDict['not sexist']['recall']:.2f}\",\n",
    "                \"Not Sexist (F1)\": f\"{reportDict['not sexist']['f1-score']:.2f}\",\n",
    "                \"Weighted (P)\": f\"{reportDict['weighted avg']['precision']:.2f}\",\n",
    "                \"Weighted (R)\": f\"{reportDict['weighted avg']['recall']:.2f}\",\n",
    "                \"Weighted (F1)\": f\"{reportDict['weighted avg']['f1-score']:.2f}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"Finished with: {full_name}\")\n",
    "\n",
    "            #Not sure about format so I am not printing this\n",
    "            #print(report)\n",
    "            \n",
    "            # Update best F1 score tracker\n",
    "            if f1_weighted > best_result[0]:\n",
    "                best_result[0] = f1_weighted\n",
    "                best_result[1] = full_name\n",
    "                best_result[2] = y_pred \n",
    "                best_result[3] = y_true_final \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {full_name}: {e}\")\n",
    "            \n",
    "\n",
    "# XGBOOST loop\n",
    "for feat_name, X_tr, X_te, y_tr, y_te in feature_sets:\n",
    "    full_name = f\"{feat_name} + XGBoost\"\n",
    "\n",
    "    print(f\"Started on: {full_name}\")\n",
    "    \n",
    "    y_true_final = y_test # The true labels are always y_test\n",
    "    try:\n",
    "        if feat_name == \"Combined + SMOTE\":\n",
    "            y_tr_enc = y_train_balanced_encoded\n",
    "        elif feat_name == \"Combined + SMOTEENN\":\n",
    "            y_tr_enc = le.transform(y_tr) \n",
    "        else:\n",
    "            y_tr_enc = le.transform(y_tr) \n",
    "\n",
    "        \n",
    "        #I got parameters from the Grid Search\n",
    "        xgb_temp = xgb.XGBClassifier(\n",
    "            n_estimators=400, max_depth=7, learning_rate=0.05, random_state=42, eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        xgb_temp.fit(X_tr, y_tr_enc)\n",
    "        y_pred_enc = xgb_temp.predict(X_te)\n",
    "        y_pred = le.inverse_transform(y_pred_enc)\n",
    "        \n",
    "        f1_weighted = f1_score(y_true_final, y_pred, average='weighted')\n",
    "\n",
    "        #Print report\n",
    "        report = classification_report(y_true_final, y_pred)\n",
    "        \n",
    "\n",
    "        \n",
    "        #This is for the dataframe\n",
    "        reportDict = classification_report(y_true_final, y_pred, output_dict=True)\n",
    "        \n",
    "        results_list.append({\n",
    "                \"Feature + Model\": f\"{feat_name} + XGBoost\",\n",
    "                \"Sexist (P)\": f\"{reportDict['sexist']['precision']:.2f}\",\n",
    "                \"Sexist (R)\": f\"{reportDict['sexist']['recall']:.2f}\",\n",
    "                \"Sexist (F1)\": f\"{reportDict['sexist']['f1-score']:.2f}\",\n",
    "                \"Not Sexist (P)\": f\"{reportDict['not sexist']['precision']:.2f}\",\n",
    "                \"Not Sexist (R)\": f\"{reportDict['not sexist']['recall']:.2f}\",\n",
    "                \"Not Sexist (F1)\": f\"{reportDict['not sexist']['f1-score']:.2f}\",\n",
    "                \"Weighted (P)\": f\"{reportDict['weighted avg']['precision']:.2f}\",\n",
    "                \"Weighted (R)\": f\"{reportDict['weighted avg']['recall']:.2f}\",\n",
    "                \"Weighted (F1)\": f\"{reportDict['weighted avg']['f1-score']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        print(f\"Finished wish: {full_name}\")\n",
    "        #Not sure about format so I am not printing this\n",
    "        #print(report)  \n",
    "\n",
    "        \n",
    "        # Update best F1 score tracker\n",
    "        if f1_weighted > best_result[0]:\n",
    "            best_result[0] = f1_weighted\n",
    "            best_result[1] = full_name\n",
    "            best_result[2] = y_pred\n",
    "            best_result[3] = y_true_final \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {full_name}: {e}\")\n",
    "        \n",
    "print(\"Finished with all combos\")\n",
    "\n",
    "#Report Best\n",
    "best_f1, best_name, y_pred_best, y_true_best = best_result\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "display(results_df)\n",
    "\n",
    "print(f\"Best Combo: {best_name}\")\n",
    "\n",
    "if y_pred_best is not None:\n",
    "    \n",
    "    final_report = classification_report(y_true_best, y_pred_best)\n",
    "    \n",
    "    print(final_report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902c2c6-84a8-4ca6-9ce3-bc04a55d9e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e03bc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\\nimport seaborn as sns\\n\\nsvm_model_cv = LinearSVC(class_weight=\"balanced\", C=0.5)\\ncv_scores = cross_val_score(svm_model_cv, X_train, y_train, cv=5)\\nprint(f\"SVM Cross-Validation Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\\n\\n\\nlog_model_cv = LogisticRegressionCV(max_iter=1000, class_weight=\"balanced\", cv=5)\\nlog_cv_scores = cross_val_score(log_model_cv, X_train, y_train, cv=5)\\nprint(f\"Logistic Regression Cross-Validation Accuracy: {np.mean(log_cv_scores):.4f} ± {np.std(log_cv_scores):.4f}\")\\ny_pred_log = log_reg.predict(X_test)\\nprint(\"Weighted F1:\", f1_score(y_test, y_pred_log, average=\"weighted\"))\\ny_pred_svm = svm_model.predict(X_test)\\nprint(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred_log))\\nprint(\"SVM Report:\\n\", classification_report(y_test, y_pred_svm))\\n\"\"\"\\nrf_model_cv = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\\nrf_cv_scores = cross_val_score(rf_model_cv, X_train, y_train, cv=5)\\nprint(f\"Random Forest Cross-Validation Accuracy: {np.mean(rf_cv_scores):.4f} ± {np.std(rf_cv_scores):.4f}\")\\ny_pred_rf = rf_model.predict(X_test)\\nprint(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\\n\"\"\"\\n# Confusion matrix for visualization\\ncm = confusion_matrix(y_test, y_pred_log, labels=log_reg.classes_)\\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\\n            xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)\\nplt.title(\"Confusion Matrix - Logistic Regression\")\\nplt.xlabel(\"Predicted\")\\nplt.ylabel(\"True\")\\nplt.show()\\n# get dict form of the report\\nrep_lr = classification_report(y_test, y_pred_log, output_dict=True)\\nweighted_lr = rep_lr[\\'weighted avg\\']   # dict with precision, recall, f1-score, support\\n\\nrep_svm = classification_report(y_test, y_pred_svm, output_dict=True)\\nweighted_svm = rep_svm[\\'weighted avg\\']\\nresults = pd.DataFrame({\\n    \"Model\": [\"Logistic Regression\", \"SVM\"],\\n    \"Accuracy\": [log_reg_score, svm_score],\\n    \"CV Mean\": [np.nan, np.mean(cv_scores)],\\n    \"CV Std\": [np.nan, np.std(cv_scores)],\\n    \"LogReg weighted P/R/F1:\": [f\"{weighted_lr[\\'precision\\']:.4f}/{weighted_lr[\\'recall\\']:.4f}/{weighted_lr[\\'f1-score\\']:.4f}\", \"\"],\\n    \"SVM weighted P/R/F1:\": [\"\", f\"{weighted_svm[\\'precision\\']:.4f}/{weighted_svm[\\'recall\\']:.4f}/{weighted_svm[\\'f1-score\\']:.4f}\"]\\n})\\nresults\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the old way I was doing it, I was too lazy to get rid of in case\n",
    "# i wanted to reuse, just ignore this cell\n",
    "# train and evaluate SVM with cross-validation\n",
    "\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "svm_model_cv = LinearSVC(class_weight=\"balanced\", C=0.5)\n",
    "cv_scores = cross_val_score(svm_model_cv, X_train, y_train, cv=5)\n",
    "print(f\"SVM Cross-Validation Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "\n",
    "log_model_cv = LogisticRegressionCV(max_iter=1000, class_weight=\"balanced\", cv=5)\n",
    "log_cv_scores = cross_val_score(log_model_cv, X_train, y_train, cv=5)\n",
    "print(f\"Logistic Regression Cross-Validation Accuracy: {np.mean(log_cv_scores):.4f} ± {np.std(log_cv_scores):.4f}\")\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "print(\"Weighted F1:\", f1_score(y_test, y_pred_log, average=\"weighted\"))\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "print(\"Logistic Regression Report:\\n\", classification_report(y_test, y_pred_log))\n",
    "print(\"SVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "\"\"\"\n",
    "rf_model_cv = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42)\n",
    "rf_cv_scores = cross_val_score(rf_model_cv, X_train, y_train, cv=5)\n",
    "print(f\"Random Forest Cross-Validation Accuracy: {np.mean(rf_cv_scores):.4f} ± {np.std(rf_cv_scores):.4f}\")\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\"\"\"\n",
    "# Confusion matrix for visualization\n",
    "cm = confusion_matrix(y_test, y_pred_log, labels=log_reg.classes_)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=log_reg.classes_, yticklabels=log_reg.classes_)\n",
    "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "# get dict form of the report\n",
    "rep_lr = classification_report(y_test, y_pred_log, output_dict=True)\n",
    "weighted_lr = rep_lr['weighted avg']   # dict with precision, recall, f1-score, support\n",
    "\n",
    "rep_svm = classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "weighted_svm = rep_svm['weighted avg']\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM\"],\n",
    "    \"Accuracy\": [log_reg_score, svm_score],\n",
    "    \"CV Mean\": [np.nan, np.mean(cv_scores)],\n",
    "    \"CV Std\": [np.nan, np.std(cv_scores)],\n",
    "    \"LogReg weighted P/R/F1:\": [f\"{weighted_lr['precision']:.4f}/{weighted_lr['recall']:.4f}/{weighted_lr['f1-score']:.4f}\", \"\"],\n",
    "    \"SVM weighted P/R/F1:\": [\"\", f\"{weighted_svm['precision']:.4f}/{weighted_svm['recall']:.4f}/{weighted_svm['f1-score']:.4f}\"]\n",
    "})\n",
    "results\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
